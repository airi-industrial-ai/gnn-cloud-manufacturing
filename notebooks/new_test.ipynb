{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08e4a93",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dmitrii\\miniconda3\\envs\\graph\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm.auto import trange\n",
    "import torch.nn as nn\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "from cloudmanufacturing.data import read_fatahi_dataset\n",
    "from cloudmanufacturing.graph import dglgraph, graph_gamma, os_type, so_type, ss_type\n",
    "from cloudmanufacturing.graphconv import GNN\n",
    "from cloudmanufacturing.solvers.mip_solver import mip_solve\n",
    "from cloudmanufacturing.validation import construct_delta, objvalue, construct_delta2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca03e536",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:02<00:00,  7.62it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = read_fatahi_dataset(\"../data/fatahi.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513ec52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dglgraph_fixed(graph, oper_max=20):\n",
    "        ncolumns = graph.ndata['feat']['o'].shape[1]\n",
    "        graph.ndata['feat'] = {'o': F.pad(graph.ndata['feat']['o'], [0, oper_max - ncolumns])}\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c88788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, problems, gammas, deltas):\n",
    "        self.problems = problems\n",
    "        self.gammas = gammas\n",
    "        self.deltas = deltas\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.problems)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        problem = self.problems[idx]\n",
    "        gamma = self.gammas[idx]\n",
    "        delta = self.deltas[idx]\n",
    "\n",
    "        graph = dglgraph(problem, gamma, delta).to(device)\n",
    "        graph = dglgraph_fixed(graph)\n",
    "        # graph.edata[\"feat\"][os_type][:, 0] /= 10\n",
    "        # graph.edata[\"feat\"][ss_type] /= 100\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70891ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_idx(string):\n",
    "    number = re.search(r'\\d+', string).group()\n",
    "    return int(number)\n",
    "\n",
    "solved_indexes = np.unique([parse_idx(l) for l in glob('solutions/*')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b461a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {}\n",
    "for idx in solved_indexes:\n",
    "    if f'solutions\\\\gamma_{idx}.npy' in glob('solutions\\\\*'):\n",
    "        info[idx] = {}\n",
    "        info[idx]['gamma'] = np.load(f'solutions\\\\gamma_{idx}.npy')\n",
    "        info[idx]['delta'] = np.load(f'solutions\\\\delta_{idx}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d4eb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = train_test_split(solved_indexes,\n",
    "                                       random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd8f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GraphDataset([dataset[i] for i in train_idx],\n",
    "                             [info[i]['gamma'] for i in train_idx],\n",
    "                             [info[i]['delta'] for i in train_idx])\n",
    "\n",
    "test_dataset = GraphDataset([dataset[i] for i in test_idx],\n",
    "                            [info[i]['gamma'] for i in test_idx],\n",
    "                            [info[i]['delta'] for i in test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3268af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = GraphDataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "test_loader = GraphDataLoader(test_dataset, batch_size=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8aee75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_objective(model, dataset):\n",
    "    test_objvalue = []\n",
    "    for i in range(len(dataset.problems)):\n",
    "        pred_gamma, pred_delta = model.predict(dataset.__getitem__(i), dataset.problems[i])\n",
    "        test_objvalue.append(\n",
    "            objvalue(dataset.problems[i], pred_gamma, pred_delta)\n",
    "        )\n",
    "    return np.mean(test_objvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bfc09ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses_info):\n",
    "    keys = [key for key in losses_info.keys() if 'loss' in key]\n",
    "    for key in keys:\n",
    "        plt.plot(losses_info[key], label=key)\n",
    "        print(f'min {key} loss: ', min(losses_info[key]))\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_objvalue(losses_info):\n",
    "    keys = [key for key in losses_info.keys() if 'objvalue' in key]\n",
    "    for key in keys:\n",
    "        plt.plot(losses_info[key], label=key)\n",
    "        plt.legend()\n",
    "        print(f'min {key} objvalue: ', min(losses_info[key]))\n",
    "    plt.show()\n",
    "\n",
    "def print_info(info):\n",
    "    plot_loss(info)\n",
    "    plot_objvalue(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fe22071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "from dgl.sampling import sample_neighbors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from cloudmanufacturing.graph import ss_type, os_type, so_type\n",
    "import numpy as np\n",
    "def cat_s_ss(edges):\n",
    "    return {'s_ss': torch.cat([edges.src['s_feat'], edges.data['feat']], dim=1)}\n",
    "\n",
    "def cat_o_os(edges):\n",
    "    return {'o_os': torch.cat([edges.src['o_feat'], edges.data['feat']], dim=1)}\n",
    "\n",
    "def cat_h_os_s(edges):\n",
    "    return {'h_os_s': torch.cat([edges.data['h_os'], edges.dst['h_s']], dim=1)}\n",
    "\n",
    "def cat_h_ss_s(edges):\n",
    "    return {'h_ss_s': torch.cat([edges.data['h_ss'], edges.dst['h_s']], dim=1)}\n",
    "\n",
    "def edge_den_os(nodes):\n",
    "    return {'den_os': torch.sum(torch.exp(nodes.mailbox['e_os']), dim=1)}\n",
    "\n",
    "def edge_den_ss(nodes):\n",
    "    return {'den_ss': torch.sum(torch.exp(nodes.mailbox['e_ss']), dim=1)}\n",
    "\n",
    "def sample_so(graph, logits):\n",
    "    with graph.local_scope():\n",
    "        graph.edata['prob'] = {'so': torch.sigmoid(logits)}\n",
    "        subg = sample_neighbors(\n",
    "            graph, \n",
    "            nodes={'o': graph.nodes('o')}, \n",
    "            fanout={'backward': 0, 'forward': 0, 'os': 0, 'so': 1, 'ss': 0},\n",
    "            prob='prob'\n",
    "        )\n",
    "        return subg.edges(etype='so')\n",
    "\n",
    "class AttnConvLayer(nn.Module):\n",
    "    def __init__(self, s_shape, o_shape,\n",
    "                 os_shape, ss_shape, out_dim):\n",
    "        super().__init__()\n",
    "        self.delta = nn.Linear(out_dim, 10)\n",
    "        self.W_s = nn.Linear(s_shape, out_dim)\n",
    "        self.W_os = nn.Linear(os_shape, out_dim)\n",
    "        self.W_ss = nn.Linear(ss_shape, out_dim)\n",
    "        self.attn = nn.Linear(out_dim * 2, 1)\n",
    "        self.W_in = nn.Linear(o_shape, out_dim)\n",
    "        self.W_self = nn.Linear(o_shape, out_dim)\n",
    "        self.W_out = nn.Linear(o_shape, out_dim)\n",
    "        self.W_o = nn.Linear(out_dim*3, out_dim)\n",
    "\n",
    "    def forward(self, graph, s_feat, o_feat):\n",
    "        with graph.local_scope():\n",
    "            z = self._conv_z(graph, s_feat, o_feat)\n",
    "            x = self._conv_x(graph, o_feat)\n",
    "            print('\\n')\n",
    "            delta_logits = self._conv_delta(graph)\n",
    "        return z, x, delta_logits\n",
    "\n",
    "    def _conv_delta(self, graph):\n",
    "        return self.delta(torch.relu(graph.edata['h_ss'][ss_type]))\n",
    "\n",
    "    def _conv_z(self, graph, s_feat, o_feat):\n",
    "        graph.ndata['s_feat'] = {'s': s_feat}\n",
    "        graph.ndata['o_feat'] = {'o': o_feat}\n",
    "\n",
    "        graph.apply_edges(cat_s_ss, etype='ss')\n",
    "        graph.apply_edges(cat_o_os, etype='os')\n",
    "\n",
    "        graph.edata['h_ss'] = {'ss': self.W_ss(graph.edata['s_ss'][ss_type])}\n",
    "        graph.edata['h_os'] = {'os': self.W_os(graph.edata['o_os'][os_type])}\n",
    "        \n",
    "        graph.ndata['h_s'] = {'s': self.W_s(s_feat)}\n",
    "        \n",
    "\n",
    "        graph.apply_edges(cat_h_ss_s, etype='ss')\n",
    "        graph.apply_edges(cat_h_os_s, etype='os')\n",
    "\n",
    "        graph.edata['e_ss'] = {'ss': F.leaky_relu(self.attn(graph.edata['h_ss_s'][ss_type]))}\n",
    "        graph.edata['e_os'] = {'os': F.leaky_relu(self.attn(graph.edata['h_os_s'][os_type]))}\n",
    "        \n",
    "        graph.multi_update_all({\n",
    "            'os': (fn.copy_e('e_os', 'e_os'), edge_den_os),\n",
    "            'ss': (fn.copy_e('e_ss', 'e_ss'), edge_den_ss),\n",
    "        }, 'sum')\n",
    "\n",
    "        graph.edata['nom_os'] = {'os': torch.exp(graph.edata['e_os'][os_type])}\n",
    "        \n",
    "        graph.edata['nom_ss'] = {'ss': torch.exp(graph.edata['e_ss'][ss_type])}\n",
    "\n",
    "        graph.apply_edges(fn.e_div_v('nom_os', 'den_os', 'alpha_os'), etype='os')\n",
    "        graph.apply_edges(fn.e_div_v('nom_ss', 'den_ss', 'alpha_ss'), etype='ss')\n",
    "\n",
    "        graph.edata['alpha_h_ss'] = {'ss': graph.edata['alpha_ss'][ss_type] * graph.edata['h_ss'][ss_type]}\n",
    "        graph.edata['alpha_h_os'] = {'os': graph.edata['alpha_os'][os_type] * graph.edata['h_os'][os_type]}\n",
    "\n",
    "        graph.multi_update_all({\n",
    "            'ss': (fn.copy_e('alpha_h_ss', 'alpha_h_ss'), fn.sum('alpha_h_ss', 'z_ss')),\n",
    "            'os': (fn.copy_e('alpha_h_os', 'alpha_h_os'), fn.sum('alpha_h_os', 'z_os')),\n",
    "        }, 'sum')\n",
    "        print(graph.ndata['z_ss']['s'])\n",
    "        z = graph.ndata['z_ss']['s'] + graph.ndata['z_os']['s']\n",
    "        return z\n",
    "\n",
    "    def _conv_x(self, graph, o_feat):\n",
    "        graph.ndata['h_in'] = {'o': self.W_in(o_feat)}\n",
    "        graph.ndata['h_self'] = {'o': self.W_self(o_feat)}\n",
    "        graph.ndata['h_out'] = {'o': self.W_out(o_feat)}\n",
    "\n",
    "        graph.multi_update_all({\n",
    "            'forward': (fn.copy_u('h_in', 'h_in'), fn.sum('h_in', 'h_in')),\n",
    "            'backward': (fn.copy_u('h_out', 'h_out'), fn.sum('h_out', 'h_out')),\n",
    "        }, 'sum')\n",
    "\n",
    "        x = torch.cat([\n",
    "            graph.ndata['h_in']['o'],\n",
    "            graph.ndata['h_self']['o'],\n",
    "            graph.ndata['h_out']['o'],\n",
    "        ], dim=1)\n",
    "\n",
    "        x = self.W_o(torch.relu(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DotProductDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, graph, z, x):\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['z'] = {'s': z}\n",
    "            graph.ndata['x'] = {'o': x}\n",
    "            graph.apply_edges(fn.u_dot_v('z', 'x', 'dot'), etype='so')\n",
    "            logits = graph.edata['dot'][so_type]\n",
    "            return logits\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self,s_shape, o_shape, os_shape,\n",
    "                 ss_shape, out_dim, n_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        os_shape = o_shape + os_shape\n",
    "        ss_shape = s_shape + ss_shape\n",
    "\n",
    "        convs = [AttnConvLayer(s_shape, o_shape,\n",
    "                               os_shape, ss_shape, out_dim)]\n",
    "        for _ in range(n_layers-1):\n",
    "            os_shape = out_dim + os_shape\n",
    "            ss_shape = out_dim + ss_shape\n",
    "            convs.append(AttnConvLayer(out_dim, out_dim,\n",
    "                                       os_shape, ss_shape, out_dim))\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.dec = DotProductDecoder()\n",
    "\n",
    "    def forward(self, graph):\n",
    "        s_feat = graph.ndata['feat']['s']\n",
    "        o_feat = graph.ndata['feat']['o']\n",
    "        s_hid, o_hid, delta_logits = self.convs[0](graph, s_feat, o_feat)\n",
    "        for conv in self.convs[1:]:\n",
    "            s_hid, o_hid, delta_logits = conv(graph, torch.relu(s_hid), torch.relu(o_hid))\n",
    "        logits = self.dec(graph, s_hid, o_hid)\n",
    "        return logits, delta_logits\n",
    "\n",
    "    def predict(self, graph, problem):\n",
    "        logits, delta_logits = self.forward(graph)\n",
    "        s, o = sample_so(graph, logits)\n",
    "        operation_index = graph.ndata['operation_index']['o'][o]\n",
    "        gamma = np.zeros(\n",
    "            (problem['n_suboperations'], problem['n_operations'], problem['n_cities'])\n",
    "        )\n",
    "        for i in range(len(operation_index)):\n",
    "            operation, task, city = operation_index[i, 1], operation_index[i, 0], s[i]\n",
    "            gamma[operation, task, city] = 1\n",
    "\n",
    "        delta = np.zeros(\n",
    "            (problem['n_services'], problem['n_cities'], problem['n_cities'],\n",
    "             problem['n_suboperations'], problem['n_operations'])\n",
    "        )\n",
    "        for i in range(len(operation_index)-1):\n",
    "            if operation_index[i][0] == operation_index[i+1][0]:\n",
    "                edge_idx = graph.edge_ids(s[i],s[i+1],etype=ss_type)\n",
    "                serv = torch.argmax(F.softmax(delta_logits[edge_idx], dim=0))\n",
    "                delta[serv, s[i], s[i+1], operation_index[i+1][1], operation_index[i][0]] = 1\n",
    "        return gamma, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "739abfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(s_shape=1, o_shape=20, os_shape=2,\n",
    "            ss_shape=10, out_dim=15, n_layers=1).to(device)\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-3, weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "afe21865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(graph_loader, dataset, model, num_epoch,\n",
    "                info={}, suffix='train', train=True):\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    loss_list = []\n",
    "    objvalue = []\n",
    "    for epoch in trange(num_epoch):\n",
    "        ep_loss = []\n",
    "        for graph in graph_loader:\n",
    "            optim.zero_grad()\n",
    "            # take initial info\n",
    "            gamma_target = graph.edata[\"target\"][os_type]\n",
    "            delta_target = graph.edata[\"delta_target\"][ss_type]\n",
    "            mask = graph.edata[\"mask\"][ss_type]\n",
    "            # Forward pass\n",
    "            logits, delta_logits = model(graph)\n",
    "            # loss calculation\n",
    "            operation_loss = F.binary_cross_entropy_with_logits(logits, gamma_target)\n",
    "            service_loss = F.cross_entropy(delta_logits[mask], delta_target[mask])\n",
    "            loss = operation_loss + service_loss\n",
    "\n",
    "            if train:\n",
    "                # Backward pass\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "        # loss calculation\n",
    "            ep_loss.append(loss.item())\n",
    "        loss_list.append(np.mean(ep_loss))\n",
    "        \n",
    "\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            objvalue.append(validate_objective(model, dataset))\n",
    "    info[f'{suffix}_loss'] = loss_list\n",
    "    info[f'{suffix}_objvalue'] = objvalue\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aa918309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:02,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:02,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:00<00:02,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:01<00:01,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:01<00:01,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:01<00:01,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<GSpMMBackward>)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 2\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[1;32mIn[122], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(graph_loader, dataset, model, num_epoch, info, suffix, train)\u001b[0m\n\u001b[0;32m     14\u001b[0m mask \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39medata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m][ss_type]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m logits, delta_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# loss calculation\u001b[39;00m\n\u001b[0;32m     18\u001b[0m operation_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(logits, gamma_target)\n",
      "File \u001b[1;32mc:\\Users\\dmitrii\\miniconda3\\envs\\graph\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dmitrii\\miniconda3\\envs\\graph\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[120], line 157\u001b[0m, in \u001b[0;36mGNN.forward\u001b[1;34m(self, graph)\u001b[0m\n\u001b[0;32m    155\u001b[0m s_feat \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeat\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    156\u001b[0m o_feat \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeat\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 157\u001b[0m s_hid, o_hid, delta_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo_feat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    159\u001b[0m     s_hid, o_hid, delta_logits \u001b[38;5;241m=\u001b[39m conv(graph, torch\u001b[38;5;241m.\u001b[39mrelu(s_hid), torch\u001b[38;5;241m.\u001b[39mrelu(o_hid))\n",
      "File \u001b[1;32mc:\\Users\\dmitrii\\miniconda3\\envs\\graph\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dmitrii\\miniconda3\\envs\\graph\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[120], line 53\u001b[0m, in \u001b[0;36mAttnConvLayer.forward\u001b[1;34m(self, graph, s_feat, o_feat)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph, s_feat, o_feat):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mlocal_scope():\n\u001b[1;32m---> 53\u001b[0m         z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_z\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo_feat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_x(graph, o_feat)\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[120], line 81\u001b[0m, in \u001b[0;36mAttnConvLayer._conv_z\u001b[1;34m(self, graph, s_feat, o_feat)\u001b[0m\n\u001b[0;32m     78\u001b[0m graph\u001b[38;5;241m.\u001b[39medata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me_ss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mss\u001b[39m\u001b[38;5;124m'\u001b[39m: F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(graph\u001b[38;5;241m.\u001b[39medata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh_ss_s\u001b[39m\u001b[38;5;124m'\u001b[39m][ss_type]))}\n\u001b[0;32m     79\u001b[0m graph\u001b[38;5;241m.\u001b[39medata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me_os\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos\u001b[39m\u001b[38;5;124m'\u001b[39m: F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(graph\u001b[38;5;241m.\u001b[39medata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh_os_s\u001b[39m\u001b[38;5;124m'\u001b[39m][os_type]))}\n\u001b[1;32m---> 81\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_update_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_e\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43me_os\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43me_os\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_den_os\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_e\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43me_ss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43me_ss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_den_ss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m graph\u001b[38;5;241m.\u001b[39medata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnom_os\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mexp(graph\u001b[38;5;241m.\u001b[39medata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me_os\u001b[39m\u001b[38;5;124m'\u001b[39m][os_type])}\n\u001b[0;32m     88\u001b[0m graph\u001b[38;5;241m.\u001b[39medata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnom_ss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mss\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mexp(graph\u001b[38;5;241m.\u001b[39medata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me_ss\u001b[39m\u001b[38;5;124m'\u001b[39m][ss_type])}\n",
      "File \u001b[1;32mc:\\Users\\dmitrii\\miniconda3\\envs\\graph\\lib\\site-packages\\dgl\\heterograph.py:5257\u001b[0m, in \u001b[0;36mDGLGraph.multi_update_all\u001b[1;34m(self, etype_dict, cross_reducer, apply_node_func)\u001b[0m\n\u001b[0;32m   5255\u001b[0m     mfunc, rfunc, afunc \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m   5256\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m etype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m[etype]\n\u001b[1;32m-> 5257\u001b[0m     all_out[dtid]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage_passing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mafunc\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   5258\u001b[0m     merge_order[dtid]\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m   5259\u001b[0m         etid\n\u001b[0;32m   5260\u001b[0m     )  \u001b[38;5;66;03m# use edge type id as merge order hint\u001b[39;00m\n\u001b[0;32m   5261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dtid, frames \u001b[38;5;129;01min\u001b[39;00m all_out\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   5262\u001b[0m     \u001b[38;5;66;03m# merge by cross_reducer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dmitrii\\miniconda3\\envs\\graph\\lib\\site-packages\\dgl\\core.py:405\u001b[0m, in \u001b[0;36mmessage_passing\u001b[1;34m(g, mfunc, rfunc, afunc)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m         orig_nid \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mdstdata\u001b[38;5;241m.\u001b[39mget(NID, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 405\u001b[0m         ndata \u001b[38;5;241m=\u001b[39m \u001b[43minvoke_udf_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsgdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_nid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_nid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;66;03m# apply phase\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m afunc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dmitrii\\miniconda3\\envs\\graph\\lib\\site-packages\\dgl\\core.py:144\u001b[0m, in \u001b[0;36minvoke_udf_reduce\u001b[1;34m(graph, func, msgdata, orig_nid)\u001b[0m\n\u001b[0;32m    141\u001b[0m ndata_bkt \u001b[38;5;241m=\u001b[39m dstdata\u001b[38;5;241m.\u001b[39msubframe(node_bkt)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# order the incoming edges per node by edge ID\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m eid_bkt \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mzerocopy_to_numpy(\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_bkt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eid_bkt) \u001b[38;5;241m==\u001b[39m deg \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(node_bkt)\n\u001b[0;32m    146\u001b[0m eid_bkt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msort(eid_bkt\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mlen\u001b[39m(node_bkt), deg)), \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dmitrii\\miniconda3\\envs\\graph\\lib\\site-packages\\dgl\\heterograph.py:3416\u001b[0m, in \u001b[0;36mDGLGraph.in_edges\u001b[1;34m(self, v, form, etype)\u001b[0m\n\u001b[0;32m   3343\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the incoming edges of the given nodes.\u001b[39;00m\n\u001b[0;32m   3344\u001b[0m \n\u001b[0;32m   3345\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3413\u001b[0m \u001b[38;5;124;03mout_edges\u001b[39;00m\n\u001b[0;32m   3414\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3415\u001b[0m v \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mprepare_tensor(\u001b[38;5;28mself\u001b[39m, v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 3416\u001b[0m src, dst, eid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_etype_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m form \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src, dst, eid\n",
      "File \u001b[1;32mc:\\Users\\dmitrii\\miniconda3\\envs\\graph\\lib\\site-packages\\dgl\\heterograph_index.py:633\u001b[0m, in \u001b[0;36mHeteroGraphIndex.in_edges\u001b[1;34m(self, etype, v)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21min_edges\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, v):\n\u001b[0;32m    613\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the in edges of the node(s).\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \n\u001b[0;32m    615\u001b[0m \u001b[38;5;124;03m    Assume that node_type(v) == dst_type(etype). Thus, the ntype argument is omitted.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;124;03m        The edge ids.\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m     edge_array \u001b[38;5;241m=\u001b[39m \u001b[43m_CAPI_DGLHeteroInEdges_2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dgl_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m     src \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mfrom_dgl_nd(edge_array(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    635\u001b[0m     dst \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mfrom_dgl_nd(edge_array(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\dmitrii\\miniconda3\\envs\\graph\\lib\\site-packages\\dgl\\_ffi\\_ctypes\\function.py:213\u001b[0m, in \u001b[0;36mFunctionBase.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    210\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m DGLValue()\n\u001b[0;32m    211\u001b[0m ret_tcode \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[0;32m    212\u001b[0m check_call(\n\u001b[1;32m--> 213\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDGLFuncCall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtcodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret_tcode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m )\n\u001b[0;32m    222\u001b[0m _ \u001b[38;5;241m=\u001b[39m temp_args\n\u001b[0;32m    223\u001b[0m _ \u001b[38;5;241m=\u001b[39m args\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "info = train_model(train_loader, train_dataset, model, 10)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    info = train_model(test_loader, test_dataset, model, 10,\n",
    "                       info=info, suffix='test', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6bdd863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min train_loss loss:  nan\n",
      "min test_loss loss:  nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min train_objvalue objvalue:  321479.3808133016\n",
      "min test_objvalue objvalue:  330030.409961122\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_info(info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
